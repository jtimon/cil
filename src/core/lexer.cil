mode lib

LANG_NAME := "cil"

TokenType := enum {
    // basic
    Eof,

    // Single-character tokens.
    Minus, Plus, Slash, Star,
    LeftParen, RightParen, LeftBrace, RightBrace, LeftBracket, RightBracket,
    Comma, Colon,

    // One or two character tokens.
    Dot, DoubleDot,
    Not, NotEqual,
    Equal, EqualEqual,
    Greater, GreaterEqual,
    Lesser, LesserEqual,
    Semicolon, DoubleSemicolon,

    // Literals.
    Identifier, String, Number,

    // Reserved words:
    Mut,

    // bool
    True,

    // type definition
    Struct, Enum,

    // function definition
    Returns, Throws,

    // flow control
    If, Else,
    While, For, In,
    Match, Switch, Default,
    Return, Throw,
    Try, Catch,

    // Special in this language:
    Mode,
    Func, Proc, Macro,
    FuncExt, ProcExt,

    // Errors (unsupported tokens)
    Const, Var,
    Fn,
    Case,
    Invalid,
    UnterminatedString,
    // UnterminatedComment // TODO do nesting comments like jai and odin, shouldn't be that hard.
}

Token := struct {
    mut token_type: TokenType = TokenType.Invalid
    // mut token_str: Str = "" // TODO re-enable strings as fields
    mut line: I64 = 0
    mut col: I64 = 0
}

// TODO implement struct arrays and add the tokens to it, for now just print the tokens
scan_push_token := proc(token_type: TokenType, token_str: Str, line: I64, col: I64) {
    mut t := Token()
    t.token_type = token_type
    // t.token_str = token_str // TODO re-enable strings as fields
    t.line = line
    t.col = col

    println("Token (", I64.to_str(line), ", ", I64.to_str(col), "): Type: ", enum_to_str(t.token_type), " str: ", token_str)
    // return t
}

is_digit := func(source: Str, pos: I64) returns Bool {
    current_char := Str.get_substr(source, pos, add(pos, 1))
    // TODO implement str_char_in_range
    switch current_char {
    case "0": return true
    case "1": return true
    case "2": return true
    case "3": return true
    case "4": return true
    case "5": return true
    case "6": return true
    case "7": return true
    case "8": return true
    case "9": return true
    case:
    }
    return false
}

is_id_start := func(source: Str, pos: I64) returns Bool {
    current_char := Str.get_substr(source, pos, add(pos, 1))
    // TODO implement str_char_in_range
    switch current_char {
    case "a": return true
    case "b": return true
    case "c": return true
    case "d": return true
    case "e": return true
    case "f": return true
    case "g": return true
    case "h": return true
    case "i": return true
    case "j": return true
    case "k": return true
    case "l": return true
    case "m": return true
    case "n": return true
    case "o": return true
    case "p": return true
    case "q": return true
    case "r": return true
    case "s": return true
    case "t": return true
    case "u": return true
    case "v": return true
    case "w": return true
    case "x": return true
    case "y": return true
    case "z": return true
    case "A": return true
    case "B": return true
    case "C": return true
    case "D": return true
    case "E": return true
    case "F": return true
    case "G": return true
    case "H": return true
    case "I": return true
    case "J": return true
    case "K": return true
    case "L": return true
    case "M": return true
    case "N": return true
    case "O": return true
    case "P": return true
    case "Q": return true
    case "R": return true
    case "S": return true
    case "T": return true
    case "U": return true
    case "V": return true
    case "W": return true
    case "X": return true
    case "Y": return true
    case "Z": return true
    case "_": return true
    case:
    }
    return false
}

scan_reserved_words := func(identifier: Str) returns TokenType {
    switch identifier {
    case "mode": return TokenType.Mode
        // declaration/arg modifiers
    case "mut": return TokenType.Mut
        // bool literals
    case "true": return TokenType.True
    case "false": return TokenType.False
        // core data types
    case "enum": return TokenType.Enum
    case "struct": return TokenType.Struct
        // function declaration
    case "returns": return TokenType.Returns
        // Anything that can be thrown must be explicitly declared in the function via 'throws', java style.
        // Except perhaps PanicException or something like that which can be implicit, but still allowed to documment redundantly
        // or perhaps not, for that may degenerate in an extra warning option
        // perhaps just force the user to explicitly catch and exit any potential panic from the callee
    case "throws": return TokenType.Throws // TODO implement
    case "func": return TokenType.Func
    case "proc": return TokenType.Proc
    case "macro": return TokenType.Macro // TODO implement for real once we compile
    case "ext_func": return TokenType.FuncExt // this have to link when we compile
    case "ext_proc": return TokenType.ProcExt // this have to link when we compile

        // control flow
    case "if": return TokenType.If
    case "else": return TokenType.Else
    case "while": return TokenType.While
    case "for": return TokenType.For // TODO
    case "in": return TokenType.In // TODO, or just use semicolon reserve forbid this
    case "switch": return TokenType.Switch
    case "match": return TokenType.Match // TODO like switch but special for declarations/assignments
    case "case": return TokenType.Case
    case "default": return TokenType.Default // TODO currently using "case:", but "default:" is more traditional, grepable and overt
    case "return": return TokenType.Return
    case "throw": return TokenType.Throw // TODO
        // TODO throw should just act as a return that gets post-processed by the next catch or rethrown
    case "catch": return TokenType.Catch
    case "try": return TokenType.Try // TODO don't allow it to open contexts, just mandatory 'try:' in any line that may throw
        // or should 'try:' be optional?

        // Reserved forbidden/illegal words (intentionally unsupported reserved words)
        // TODO intentionally unsupport more reserved words
        // TODO nicer messages for forbidden words
    case "fn": return TokenType.Fn
    case "function": return TokenType.Invalid
    case "method": return TokenType.Invalid
    case "global": return TokenType.Invalid // just use mut declaration in the root of the file, but they're not allowed in all modes
        // const/vars are the most abstract types, you can't even explicitly declare them
    case "const": return TokenType.Const
    case "var": return TokenType.Var
        // Do we really need const fields static other than static? (ie can be different per instance, but not modified afterwards)
        // The answer is probably yet, but perhaps static is not the right answer
        // how about this? if it's in the struct body, it is const, if it is in impl, it is static, just like functions\
        // or do we need mut function fields too? probably yes
    case "static": return TokenType.Invalid
    case:
    }
    return TokenType.Identifier
}

scan_tokens := proc(source: Str) {
    eof_pos := Str.len(source) // TODO fix
    // eof_pos := source.len()

    mut pos := 0
    mut line := 0
    mut start_line_pos := 0

    while lt(pos, eof_pos) {
        start := pos
        if is_digit(source, pos) {
            while and(lt(pos, eof_pos), is_digit(source, pos)) {
                pos.inc()
            }
            // Look for a fractional part.
            if and(Str.eq(Str.get_substr(source, pos, add(pos, 1)), "."), is_digit(source, add(pos, 1))) {
                pos.inc()
                while and(lt(pos, eof_pos), is_digit(source, pos)) {
                    pos.inc()
                }
            }
            scan_push_token(TokenType.Number, Str.get_substr(source, start, pos), line, sub(pos, start_line_pos))
        } else {

            current_char := Str.get_substr(source, pos, add(pos, 1))
            switch current_char {
                // chars to ignore in this language:
            case " ":  pos.inc()
            case "\n": pos.inc()
            case "\r": pos.inc()
            case "\t": pos.inc()

                // open/close. left/right parentheses, craces and brackets
            case "(":
                scan_push_token(TokenType.LeftParen, current_char, line, sub(pos, start_line_pos))
                pos.inc()
            case ")":
                scan_push_token(TokenType.RightParen, current_char, line, sub(pos, start_line_pos))
                pos.inc()
            case "{":
                scan_push_token(TokenType.LeftBrace, current_char, line, sub(pos, start_line_pos))
                pos.inc()
            case "}":
                scan_push_token(TokenType.RightBrace, current_char, line, sub(pos, start_line_pos))
                pos.inc()
            case "[":
                scan_push_token(TokenType.LeftBracket, current_char, line, sub(pos, start_line_pos))
                pos.inc()
            case "]":
                scan_push_token(TokenType.RightBracket, current_char, line, sub(pos, start_line_pos))
                pos.inc()

                // separator for optional type before the equal in declarations or args
            case ":":
                scan_push_token(TokenType.Colon, current_char, line, sub(pos, start_line_pos))
                pos.inc()
                // separator for args
            case ",":
                scan_push_token(TokenType.Comma, current_char, line, sub(pos, start_line_pos))
                pos.inc()

                // math
            case "+":
                scan_push_token(TokenType.Plus, current_char, line, sub(pos, start_line_pos))
                pos.inc()
            case "-":
                scan_push_token(TokenType.Minus, current_char, line, sub(pos, start_line_pos))
                pos.inc()
            case "*":
                scan_push_token(TokenType.Star, current_char, line, sub(pos, start_line_pos))
                pos.inc()

                // reserved for two chars in a row
            case ".":
                if Str.eq(".", Str.get_substr(source, add(pos, 1), add(pos, 2))) {
                    scan_push_token(TokenType.DoubleDot, "..", line, sub(pos, start_line_pos))
                    pos.inc()
                } else {
                    scan_push_token(TokenType.Dot, current_char, line, sub(pos, start_line_pos))
                }
                pos.inc()
            case "=":
                if Str.eq("=", Str.get_substr(source, add(pos, 1), add(pos, 2))) {
                    scan_push_token(TokenType.EqualEqual, "..", line, sub(pos, start_line_pos))
                    pos.inc()
                } else {
                    scan_push_token(TokenType.Equal, current_char, line, sub(pos, start_line_pos))
                }
                pos.inc()
            case "<":
                if Str.eq("=", Str.get_substr(source, add(pos, 1), add(pos, 2))) {
                    scan_push_token(TokenType.LesserEqual, "..", line, sub(pos, start_line_pos))
                    pos.inc()
                } else {
                    scan_push_token(TokenType.Lesser, current_char, line, sub(pos, start_line_pos))
                }
                pos.inc()
            case ">":
                if Str.eq("=", Str.get_substr(source, add(pos, 1), add(pos, 2))) {
                    scan_push_token(TokenType.GreaterEqual, "..", line, sub(pos, start_line_pos))
                    pos.inc()
                } else {
                    scan_push_token(TokenType.Greater, current_char, line, sub(pos, start_line_pos))
                }
                pos.inc()
            case "!":
                if Str.eq("=", Str.get_substr(source, add(pos, 1), add(pos, 2))) {
                    scan_push_token(TokenType.NotEqual, "..", line, sub(pos, start_line_pos))
                    pos.inc()
                } else {
                    scan_push_token(TokenType.Not, current_char, line, sub(pos, start_line_pos))
                }
                pos.inc()

                // comments:
                // TODO fix end of lines, this can't work before that
            case "#":
                pos.inc()
                while and(lt(add(pos, 1), eof_pos), not(Str.eq("\n", Str.get_substr(source, pos, add(pos, 2))))) {
                    pos.inc()
                }

            case "/":
                // TODO allow the other type of commments '/*', allowing nesting like odin or jai
                if Str.eq("/", Str.get_substr(source, add(pos, 1), add(pos, 2))) {
                    pos = add(pos, 2)
                    while and(lt(add(pos, 1), eof_pos), not(Str.eq("\n", Str.get_substr(source, pos, add(pos, 2))))) {
                        pos.inc()
                    }

                } else {
                    scan_push_token(TokenType.Slash, current_char, line, sub(pos, start_line_pos))
                }
                pos.inc()

                // literal strings
                // case "\"":
            case "\"":
                pos.inc()
                while and(lt(add(pos, 1), eof_pos), not(Str.eq(Str.get_substr(source, pos, add(pos, 1)), "\""))) {
                    if Str.eq(Str.get_substr(source, pos, add(pos, 1)), "\\") {
                        pos.inc() // if it's the escape character, skip the next character too
                    }
                    pos.inc()
                }
                if gteq(pos, eof_pos) {
                    token_str := Str.get_substr(source, start, add(pos, 1))
                    scan_push_token(TokenType.UnterminatedString, token_str, line, sub(pos, add(start_line_pos, 1)))
                } else {
                    token_str := Str.get_substr(source, add(start, 1), pos)
                    scan_push_token(TokenType.String, token_str, line, sub(pos, add(start_line_pos, 1)))
                }

                // Everything else must be reserved words, identifiers or invalid
            case:
                if is_id_start(source, pos) {
                    pos.inc()
                    while and(lt(pos, eof_pos), or(is_digit(source, pos), is_id_start(source, pos))) {
                        pos.inc()
                        // TODO fix literal strings with escape characters like in the rust implementation
                    }
                    pos.dec()
                    id_str := Str.get_substr(source, start, add(pos, 1))
                    scan_push_token(scan_reserved_words(id_str), id_str, line, sub(pos, add(start_line_pos, 1)))
                } else {
                    scan_push_token(TokenType.Invalid, current_char, line, sub(pos, start_line_pos))
                }
                pos.inc()
            } // switch
        } // else
        // println("Current char:", current_char)
    } // while
    scan_push_token(TokenType.Eof, "End of file", line, 0);
}

Lexer := struct {
    mut debug_compiler : Bool = false
    mut path : Str = ""
    mut src : Str = ""
    // TODO store tokens in lexer
    // mut tokens: []Token = Array::new() // TODO
    mut current_token : I64 = 0
    mut arr_current_token := Token() // TODO stop faking tokens array

    new_fake_path := func(fake_path: Str, src: Str) returns Lexer {
        l := Lexer()
        l.path = fake_path
        // TODO lex tokens by calling scan_tokens()
        return l
    }

    new := func(path: Str) returns Lexer {
        src := readfile(path)
        return Expr.new_fake_path(path, src)
    }

    peek := func(self: Lexer) returns Token {
        return self.arr_current_token
    }

    consume := func(mut self: Lexer) returns Token {
        return I64.inc(self.current_token)
    }

    expect := func(mut self: Lexer, expected: TokenType) returns Bool {
        p := self.peek()
        switch p.token_type {
        case expected:
            self.current_token = I64.add(self.current_token, 1)
        case:
            println(loc(), "Lexer.expect: the current token is not what was expected")
            return false
        }
        return true
    }

    error := proc(self: Lexer, loc_str: Str, t: Token, msg: Str) {
        println(self.path, ":", t.line, ":", t.col, ": parse ERROR: ", msg)
        if debug_compiler {
            println(loc_str, "parse ERROR: ", msg)
        }
        exit(1)
    }

}

print_lex_error := proc(path: Str, t: Token, errors_found: I64, msg: Str) {
    print(path, ":", I64.to_str(t.line), ":", I64.to_str(t.col), ": Lexical error ", I64.to_str(errors_found), ": ", msg)
    // print(". Offending symbol: '", t.token_str, "'") // TODO FIX Str fields in structs
    print("\n")
    errors_found.inc()
}

print_if_lex_error := proc(path: Str, t: Token, mut errors_found: I64) {
    switch t.token_type {
    case TokenType.Invalid:
        print_lex_error(path, t, errors_found, "Invalid character")

    case TokenType.UnterminatedString:
        print_lex_error(path, t, errors_found, "Unterminated String\nSuggestion: add missing '\"'")

    case TokenType.Const:
        print_lex_error(path, t, errors_found, "No need to use 'const', everything is const by default unless 'mut' is used")

    case TokenType.Var:
        print_lex_error(path, t, errors_found, "Keyword 'var' is not supported\nSuggestion: use 'mut' instead")

    case TokenType.Fn:
        print_lex_error(path, t, errors_found, "Keyword 'fn' is not supported\nSuggestion: use 'func' or 'proc' instead")

    case TokenType.DoubleSemicolon:
        print_lex_error(path, t, errors_found, "No need for ';;' (aka empty statements)\nSuggestion: try 'if true {}' instead, whatever you want that for")

    case TokenType.Plus:
        print_lex_error(path, t, errors_found, "Operator '+' is not supported yet\nSuggestion: use core func 'add' instead")

    case TokenType.Minus:
        print_lex_error(path, t, errors_found, "Operator '-' is not supported yet\nSuggestion: use core func 'sub' instead")

    case TokenType.Star:
        print_lex_error(path, t, errors_found, "Operator '*' is not supported yet\nSuggestion: use core func 'mul' instead")

    case TokenType.Slash:
        print_lex_error(path, t, errors_found, "Operator '/' is not supported yet\nSuggestion: use core func 'div' instead")

    case TokenType.EqualEqual:
        print_lex_error(path, t, errors_found, "Operator '==' is not supported yet\nSuggestion: use 'I64.eq' or 'String.eq' instead")

    case TokenType.Lesser:
        print_lex_error(path, t, errors_found, "Operator '<' is not supported yet\nSuggestion: use core func 'lt' instead")

    case TokenType.LesserEqual:
        print_lex_error(path, t, errors_found, "Operator '<=' is not supported yet\nSuggestion: use core func 'lteq' instead")

    case TokenType.Greater:
        print_lex_error(path, t, errors_found, "Operator '>' is not supported yet\nSuggestion: use core func 'gt' instead")

    case TokenType.GreaterEqual:
        print_lex_error(path, t, errors_found, "Operator '>=' is not supported yet\nSuggestion: use core func 'gteq' instead")

    case TokenType.Not:
        print_lex_error(path, t, errors_found, "Operator '!' is not supported yet\nSuggestion: use core func 'not' instead")

    case TokenType.NotEqual:
        print_lex_error(path, t, errors_found, "Operator '!=' is not supported yet\nSuggestion: use core funcs 'not' and 'I64.eq'/'String.eq' instead")

    case:
        // No error, do nothing
    }
}
