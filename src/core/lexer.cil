mode lib

TokenType := enum {
    // basic
    Eof,

    // Single-character tokens.
    Minus, Plus, Slash, Star,
    LeftParen, RightParen, LeftBrace, RightBrace, LeftBracket, RightBracket,
    Comma, Colon,

    // One or two character tokens.
    Dot, DoubleDot,
    Not, NotEqual,
    Equal, EqualEqual,
    Greater, GreaterEqual,
    Lesser, LesserEqual,
    Semicolon, DoubleSemicolon,
    // TODO implemment DoubleSemicolon

    // Literals.
    Identifier, String, Number,

    // Reserved words:
    Mut,

    // bool
    True, False,
    // type definition
    Struct, Enum,
    // function definition
    Returns, Throws,
    // flow control
    If, Else,
    While, For, In,
    Match, Switch, Default,
    Return, Throw,
    Try, Catch,

    // Special in this language:
    Func, Proc, Macro,
    Mode,

    // Errors
    Const, Var,
    Fn,
    Case,
    Invalid,
    UnterminatedString,
    // UnterminatedComment // TODO do nesting comments like jai and odin, shoulnd't be that hard. ideally in the lexer itself
}

Token := struct {
    mut token_type: TokenType = TokenType.Invalid
    // mut token_str: String = "" // TODO re-enable strings as fields
    mut line: I64 = 0
    mut col: I64 = 0
}

// TODO implement struct arrays and add the tokens to it, for now just print the tokens
scan_push_token := proc(token_type: TokenType, token_str: String, line: I64, col: I64) {
    mut t := Token()
    t.token_type = token_type
    // t.token_str = token_str // TODO re-enable strings as fields
    t.line = line
    t.col = col

    println("Token (", I64.to_str(line), ", ", I64.to_str(col), "): Type: ", enum_to_str(t.token_type), " str: ", token_str)
    // return t
}

is_digit := func(source: String, pos: I64) returns Bool {
    current_char := String.get_substr(source, pos, add(pos, 1))
    // TODO implement str_char_in_range
    switch current_char {
    case "0": return true
    case "1": return true
    case "2": return true
    case "3": return true
    case "4": return true
    case "5": return true
    case "6": return true
    case "7": return true
    case "8": return true
    case "9": return true
    case:
    }
    return false
}

is_id_start := func(source: String, pos: I64) returns Bool {
    current_char := String.get_substr(source, pos, add(pos, 1))
    // TODO implement str_char_in_range
    switch current_char {
    case "a": return true
    case "b": return true
    case "c": return true
    case "d": return true
    case "e": return true
    case "f": return true
    case "g": return true
    case "h": return true
    case "i": return true
    case "j": return true
    case "k": return true
    case "l": return true
    case "m": return true
    case "n": return true
    case "o": return true
    case "p": return true
    case "q": return true
    case "r": return true
    case "s": return true
    case "t": return true
    case "u": return true
    case "v": return true
    case "w": return true
    case "x": return true
    case "y": return true
    case "z": return true
    case "A": return true
    case "B": return true
    case "C": return true
    case "D": return true
    case "E": return true
    case "F": return true
    case "G": return true
    case "H": return true
    case "I": return true
    case "J": return true
    case "K": return true
    case "L": return true
    case "M": return true
    case "N": return true
    case "O": return true
    case "P": return true
    case "Q": return true
    case "R": return true
    case "S": return true
    case "T": return true
    case "U": return true
    case "V": return true
    case "W": return true
    case "X": return true
    case "Y": return true
    case "Z": return true
    case "_": return true
    case:
    }
    return false
}

scan_reserved_words := func(identifier: String) returns TokenType {
    switch identifier {
    case "mode": return TokenType.Mode
        // declaration/arg modifiers
    case "mut": return TokenType.Mut
        // bool literals
    case "true": return TokenType.True
    case "false": return TokenType.False
        // core data types
    case "enum": return TokenType.Enum
    case "struct": return TokenType.Struct
        // function declaration
    case "returns": return TokenType.Returns
        // Anything that can be thrown must be explicitly declared in the function via 'throws', java style.
        // Except perhaps PanicException or something like that which can be implicit, but still allowed to documment redundantly
        // or perhaps not, for that may degenerate in an extra warning option
        // perhaps just force the user to explicitly catch and exit any potential panic from the callee
    case "throws": return TokenType.Throws // TODO implement
    case "func": return TokenType.Func
    case "proc": return TokenType.Proc
    case "macro": return TokenType.Macro // TODO implement for real once we compile
    case "ext_func": return TokenType.FuncExt // this have to link when we compile
    case "ext_proc": return TokenType.ProcExt // this have to link when we compile

        // control flow
    case "if": return TokenType.If
    case "else": return TokenType.Else
    case "while": return TokenType.While
    case "for": return TokenType.For // TODO
    case "in": return TokenType.In // TODO, or just use semicolon reserve forbid this
    case "switch": return TokenType.Switch
    case "match": return TokenType.Match // TODO like switch but special for declarations/assignments
    case "case": return TokenType.Case
    case "default": return TokenType.Default // TODO currently using "case:", but "default:" is more traditional, grepable and overt
    case "return": return TokenType.Return
    case "throw": return TokenType.Throw // TODO
        // TODO throw should just act as a return that gets post-processed by the next catch or rethrown
    case "catch": return TokenType.Catch
    case "try": return TokenType.Try // TODO don't allow it to open contexts, just mandatory 'try:' in any line that may throw
        // or should 'try:' be optional?

        // Reserved forbidden/illegal words (intentionally unsupported reserved words)
        // TODO intentionally unsupport more reserved words
        // TODO nicer messages for forbidden words
    case "fn": return TokenType.Fn
    case "function": return TokenType.Invalid
    case "method": return TokenType.Invalid
    case "global": return TokenType.Invalid // just use mut declaration in the root of the file, but they're not allowed in all modes
        // const/vars are the most abstract types, you can't even explicitly declare them
    case "const": return TokenType.Const
    case "var": return TokenType.Var
        // Do we really need const fields static other than static? (ie can be different per instance, but not modified afterwards)
        // The answer is probably yet, but perhaps static is not the right answer
        // how about this? if it's in the struct body, it is const, if it is in impl, it is static, just like functions\
        // or do we need mut function fields too? probably yes
    case "static": return TokenType.Invalid
    case:
    }
    return TokenType.Identifier
}

scan_tokens := proc(source: String) {
    eof_pos := String.len(source) // TODO fix
    // eof_pos := source.len()

    mut pos := 0
    mut line := 0
    mut start_line_pos := 0

    while lt(pos, eof_pos) {
        start := pos
        if is_digit(source, pos) {
            while and(lt(pos, eof_pos), is_digit(source, pos)) {
                pos.inc()
            }
            // Look for a fractional part.
            if and(String.eq(String.get_substr(source, pos, add(pos, 1)), "."), is_digit(source, add(pos, 1))) {
                pos.inc()
                while and(lt(pos, eof_pos), is_digit(source, pos)) {
                    pos.inc()
                }
            }
            scan_push_token(TokenType.Number, String.get_substr(source, start, pos), line, sub(pos, start_line_pos))
        } else {

            current_char := String.get_substr(source, pos, add(pos, 1))
            switch current_char {
                // chars to ignore in this language:
            case " ":  pos.inc()
            case "\n": pos.inc()
            case "\r": pos.inc()
            case "\t": pos.inc()

                // open/close. left/right parentheses, craces and brackets
            case "(":
                scan_push_token(TokenType.LeftParen, current_char, line, sub(pos, start_line_pos))
                pos.inc()
            case ")":
                scan_push_token(TokenType.RightParen, current_char, line, sub(pos, start_line_pos))
                pos.inc()
            case "{":
                scan_push_token(TokenType.LeftBrace, current_char, line, sub(pos, start_line_pos))
                pos.inc()
            case "}":
                scan_push_token(TokenType.RightBrace, current_char, line, sub(pos, start_line_pos))
                pos.inc()
            case "[":
                scan_push_token(TokenType.LeftBracket, current_char, line, sub(pos, start_line_pos))
                pos.inc()
            case "]":
                scan_push_token(TokenType.RightBracket, current_char, line, sub(pos, start_line_pos))
                pos.inc()

                // separator for optional type before the equal in declarations or args
            case ":":
                scan_push_token(TokenType.Colon, current_char, line, sub(pos, start_line_pos))
                pos.inc()
                // separator for args
            case ",":
                scan_push_token(TokenType.Comma, current_char, line, sub(pos, start_line_pos))
                pos.inc()

                // math
            case "+":
                scan_push_token(TokenType.Plus, current_char, line, sub(pos, start_line_pos))
                pos.inc()
            case "-":
                scan_push_token(TokenType.Minus, current_char, line, sub(pos, start_line_pos))
                pos.inc()
            case "*":
                scan_push_token(TokenType.Star, current_char, line, sub(pos, start_line_pos))
                pos.inc()

                // reserved for two chars in a row
            case ".":
                if String.eq(".", String.get_substr(source, add(pos, 1), add(pos, 2))) {
                    scan_push_token(TokenType.DoubleDot, "..", line, sub(pos, start_line_pos))
                    pos.inc()
                } else {
                    scan_push_token(TokenType.Dot, current_char, line, sub(pos, start_line_pos))
                }
                pos.inc()
            case "=":
                if String.eq("=", String.get_substr(source, add(pos, 1), add(pos, 2))) {
                    scan_push_token(TokenType.EqualEqual, "..", line, sub(pos, start_line_pos))
                    pos.inc()
                } else {
                    scan_push_token(TokenType.Equal, current_char, line, sub(pos, start_line_pos))
                }
                pos.inc()
            case "<":
                if String.eq("=", String.get_substr(source, add(pos, 1), add(pos, 2))) {
                    scan_push_token(TokenType.LesserEqual, "..", line, sub(pos, start_line_pos))
                    pos.inc()
                } else {
                    scan_push_token(TokenType.Lesser, current_char, line, sub(pos, start_line_pos))
                }
                pos.inc()
            case ">":
                if String.eq("=", String.get_substr(source, add(pos, 1), add(pos, 2))) {
                    scan_push_token(TokenType.GreaterEqual, "..", line, sub(pos, start_line_pos))
                    pos.inc()
                } else {
                    scan_push_token(TokenType.Greater, current_char, line, sub(pos, start_line_pos))
                }
                pos.inc()
            case "!":
                if String.eq("=", String.get_substr(source, add(pos, 1), add(pos, 2))) {
                    scan_push_token(TokenType.NotEqual, "..", line, sub(pos, start_line_pos))
                    pos.inc()
                } else {
                    scan_push_token(TokenType.Not, current_char, line, sub(pos, start_line_pos))
                }
                pos.inc()

                // comments:
                // TODO fix end of lines, this can't work before that
            case "#":
                pos.inc()
                while and(lt(add(pos, 1), eof_pos), not(String.eq("\n", String.get_substr(source, pos, add(pos, 2))))) {
                    pos.inc()
                }

            case "/":
                // TODO allow the other type of commments '/*', allowing nesting like odin or jai
                if String.eq("/", String.get_substr(source, add(pos, 1), add(pos, 2))) {
                    pos = add(pos, 2)
                    while and(lt(add(pos, 1), eof_pos), not(String.eq("\n", String.get_substr(source, pos, add(pos, 2))))) {
                        pos.inc()
                    }

                } else {
                    scan_push_token(TokenType.Slash, current_char, line, sub(pos, start_line_pos))
                }
                pos.inc()

                // literal strings
                // case "\"":
            case "\"":
                pos.inc()
                while and(lt(add(pos, 1), eof_pos), not(String.eq(String.get_substr(source, pos, add(pos, 1)), "\""))) {
                    if String.eq(String.get_substr(source, pos, add(pos, 1)), "\\") {
                        pos.inc() // if it's the escape character, skip the next character too
                    }
                    pos.inc()
                }
                if gteq(pos, eof_pos) {
                    token_str := String.get_substr(source, start, add(pos, 1))
                    scan_push_token(TokenType.UnterminatedString, token_str, line, sub(pos, add(start_line_pos, 1)))
                } else {
                    token_str := String.get_substr(source, add(start, 1), pos)
                    scan_push_token(TokenType.String, token_str, line, sub(pos, add(start_line_pos, 1)))
                }

                // Everything else must be reserved words, identifiers or invalid
            case:
                if is_id_start(source, pos) {
                    pos.inc()
                    while and(lt(pos, eof_pos), or(is_digit(source, pos), is_id_start(source, pos))) {
                        pos.inc()
                        // TODO fix literal strings with escape characters like in the rust implementation
                    }
                    pos.dec()
                    id_str := String.get_substr(source, start, add(pos, 1))
                    scan_push_token(scan_reserved_words(id_str), id_str, line, sub(pos, add(start_line_pos, 1)))
                } else {
                    scan_push_token(TokenType.Invalid, current_char, line, sub(pos, start_line_pos))
                }
                pos.inc()
            } // switch
        } // else
        // println("Current char:", current_char)
    } // while
    scan_push_token(TokenType.Eof, "End of file", line, 0);
}

Lexer := struct {
    // TODO store tokens in lexer
    // mut tokens: []Token = Array::new() // TODO

    new_by_src := func(src: String) returns Lexer {
        self := Lexer()
        // l.tokens = scan_tokens(src) // TODO
        return self
    }

    new_by_path := func(path: String) returns Lexer {
        src := readfile(path)
        return new_by_src(src)
    }
}
